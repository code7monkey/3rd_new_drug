# 3rd_new_drug

**Jump AI (Python) 2025 â€“ 3rd AI Drug Discovery Challenge**  
**ğŸ¥ˆ 2nd Place / 502 Teams â€“ ChemBERTa pIC50 Prediction**

---

This repository contains a **ChemBERTa-based regression pipeline** for predicting **pIC50 values from SMILES**, developed for the **ASK1 (MAP3K5) target**.

The project is designed with **clear separation between training and inference**, and all experiments are managed through **YAML configuration files** for reproducibility and flexibility.

---

## ğŸ¯ Project Goals

- **pIC50 regression from SMILES inputs**
- **Scaffold-based Stratified Group K-Fold cross-validation**
- **Stable training using HuggingFace Trainer**
- **Soft-ensemble inference using best checkpoints from each fold**

---

## ğŸ“ Project Structure

    chemberta_project/
    â”œâ”€â”€ src/                    # Core logic (importable modules)
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ model.py            # Model definition & loading
    â”‚   â”œâ”€â”€ dataset.py          # Data preprocessing & CV split
    â”‚   â”œâ”€â”€ trainer.py          # Cross-validation training loop
    â”‚   â”œâ”€â”€ losses.py           # (Optional) custom loss functions
    â”‚   â””â”€â”€ utils.py            # Shared utilities
    â”‚
    â”œâ”€â”€ train.py                # Training entry point
    â”œâ”€â”€ inference.py            # Inference & submission generation
    â”‚
    â”œâ”€â”€ configs/                # Experiment configs (YAML-based)
    â”‚   â”œâ”€â”€ train.yaml
    â”‚   â””â”€â”€ submit.yaml
    â”‚
    â”œâ”€â”€ assets/                 # Model weights / tokenizer (gitignored)
    â”‚   â”œâ”€â”€ model.pt
    â”‚   â””â”€â”€ tokenizer/
    â”‚
    â”œâ”€â”€ requirements.txt        # Fixed environment dependencies
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ .gitattributes
    â””â”€â”€ README.md

---

## ğŸ›  Environment Setup

Python **3.9+** is recommended.

    pip install -r requirements.txt

---

## ğŸ“Š Dataset Format

The dataset consists of **two columns only**: `ID` and `Smiles`.

    ID,Smiles
    TEST_000,CCO...
    TEST_001,CCN...

---

## ğŸš€ Training

### Configure Training Settings

Edit `configs/train.yaml` to control:

- Pretrained model (e.g. `DeepChem/ChemBERTa-77M-MTR`)
- Batch size, epochs, learning rate
- Number of folds
- Output directories

### Run Training

    python train.py --config configs/train.yaml

After training, the following artifacts are generated:

- Best checkpoint per fold
- Out-of-fold predictions (`oof_*.csv`)
- `manifest.json` (used for inference)

---

## ğŸ“¦ Inference & Submission

    python inference.py --config configs/submit.yaml

Inference pipeline:

- Loads best checkpoint from each fold
- Averages fold-wise pIC50 predictions (soft ensemble)
- Converts **pIC50 â†’ IC50 (nM)**
- Saves final submission file

---

## ğŸ§  Model Details

- **Backbone**: ChemBERTa-77M-MTR  
- **Task**: Regression (pIC50)  
- **Loss Function**: Mean Squared Error (MSE)  
- **Cross-Validation Strategy**:
  - Murcko scaffold-based grouping  
  - Stratification using binned pIC50 values  

---

## ğŸ“Œ Notes

- `assets/`, `data/`, and `ckpt/` directories are excluded via `.gitignore`
- Git LFS is recommended for large model files
- `losses.py` is prepared for future experiments with custom loss functions
